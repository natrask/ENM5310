{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/natrask/ENM5310/blob/main/Lecture0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP96fhyLI_Qa"
   },
   "source": [
    "## Lecture Thurs 1/25\n",
    "\n",
    "Today we're going to start building some complex models. The homework assignment for the coming week will delve into pencil and paper traditional statistical estimation for fitting simplistic models and working with probability distributions. Today however we're going to focus on how to build up some more complicated models.\n",
    "\n",
    "### Step 1: Visualize data\n",
    "\n",
    "Plot your dataset to understand trends, identify outliers, get order of estimate scaling.\n",
    "\n",
    "### Step 2: Build up a reasonable model\n",
    "Postulate a model, derive expressions for joint distribution, log likelihood function, etc.\n",
    "\n",
    "### Step 3: Implement in TF/Pytorch/JAX\n",
    "Implement model in learning architecture. Build loss function. Train with first-order optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'probml_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprobml_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpml\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'probml_utils'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall -qq git+https://github.com/probml/probml-utils.git\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprobml_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpml\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m;\n\u001b[1;32m     12\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticks\u001b[39m\u001b[38;5;124m\"\u001b[39m, color_codes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'probml_utils'"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset - see fig 2.11 in Murphy\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "import seaborn as sns;\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq pandas\n",
    "    import pandas as pd\n",
    "pd.set_option('display.precision', 2) # 2 decimal places\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.width', 100) # wide windows\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq scikit-learn\n",
    "    import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract numpy arrays\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "# Convert to pandas dataframe \n",
    "df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "df['label'] = pd.Series(iris.target_names[y], dtype='category')\n",
    "\n",
    "# we pick a color map to match that used by decision tree graphviz \n",
    "#cmap = ListedColormap(['#fafab0','#a0faa0', '#9898ff']) # orange, green, blue/purple\n",
    "#cmap = ListedColormap(['orange', 'green', 'purple']) \n",
    "palette = {'setosa': 'orange', 'versicolor': 'green', 'virginica': 'purple'}\n",
    "\n",
    "g = sns.pairplot(df, vars = df.columns[0:4], hue=\"label\", palette=palette)\n",
    "#g = sns.pairplot(df, vars = df.columns[0:4], hue=\"label\")\n",
    "pml.savefig('iris_scatterplot_purple.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Change colum names\n",
    "iris_df = df.copy()\n",
    "iris_df.columns =  ['sl', 'sw', 'pl', 'pw'] + ['label'] \n",
    "\n",
    "g = sns.pairplot(iris_df, vars = iris_df.columns[0:4], hue=\"label\")\n",
    "plt.tight_layout()\n",
    "pml.savefig('iris_pairplot.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.stripplot(x=\"label\", y=\"sl\", data=iris_df, jitter=True)\n",
    "pml.savefig('iris_sepal_length_strip_plot.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_woU4wppOeY5",
    "outputId": "f5d6889e-5b80-41bb-9179-2855bf000711"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t62BDB7AOxmj"
   },
   "source": [
    "Next we will demonstrate how to build up a tensorflow model which will implement a function of the form $f(x) = \\theta_1 x + \\theta_2$. In tensorflow 1 there are 3 fundamental objects: a constant, which is a tensor maintaining a constant value; a Variable, which is a tensor whose value will evolve via gradient descent or some other process for assignment, and a placeholder which is a tensor whose value will be provided from data as one processes through a dataset. Think of this as defining a \"circuit\" which data flows through, being loaded into placeholders and flowing to output nodes. When defining a model, the initial values of Variables must be specified, and the model must be initialized before evaluating. When specifying the dimension of a tensor the **None** keyword serves as a placeholder for an unknown length dimension (typically used for the number of data points so we can push in batches of varying size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM6EOAhePAsU"
   },
   "outputs": [],
   "source": [
    "theta1 = tf.Variable(2.0,dtype=tf.float32)\n",
    "theta2 = tf.constant(3.0,dtype=tf.float32)\n",
    "x = tf.placeholder(dtype=tf.float32,shape=(None))\n",
    "f_of_x = theta1*x+theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NM09NTwQQ2z"
   },
   "source": [
    "Data can be evaluated by defining a dictionary which specifies what values should be fed into the placeholder values. This is the mechanism by which we'll feed batches of data - we will successively redefine the feed_dict to point to different slices of the dataset. Note how the **None** keyword allows one to provide different length tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fspJAowaPS4e",
    "outputId": "2ee28ff3-0ab8-4bf1-c5f3-954068d6b2c0"
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer()) #initialize model\n",
    "myDict1 = {x:3}\n",
    "myDict2 = {x:5}\n",
    "myDict3 = {x:[1,2]}\n",
    "print('f(x) = 2*x+3')\n",
    "print('f(3) = ' + str(sess.run(f_of_x,feed_dict=myDict1)))\n",
    "print('f(5) = ' + str(sess.run(f_of_x,feed_dict=myDict2)))\n",
    "print('[f(1),f(2)] = ' + str(sess.run(f_of_x,feed_dict=myDict3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgGUr7FbTZQ1"
   },
   "source": [
    "While tensorflow is popularly known as a language for building up deep learning models, you should think of it primarily as an engine for performing automatic differentiation. Note that due to how TF calculates gradients, it is only valid to take the gradient of a scalar with respect to a tensor (i.e. no direct computation of Jacobians or Hessians). Pytorch has a similar issue, while Jax supports forward and reverse mode differentiation (i.e. derivatives of tensors with respect to tensors). Tensorflow 2 maintains a more flexible but less simple syntax for autodiff that you can read more about [here](https://www.tensorflow.org/guide/autodiff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KTdI4rbRaet",
    "outputId": "0cef7533-fcbf-4970-dccd-7a719bb93f5e"
   },
   "outputs": [],
   "source": [
    "df_dtheta1 = tf.gradients(f_of_x,x)[0]\n",
    "print('dfdx(x) = 2')\n",
    "print('df(3)/dtheta1 = ' + str(sess.run(df_dtheta1,feed_dict=myDict1)))\n",
    "print('df(5)/dtheta1 = ' + str(sess.run(df_dtheta1,feed_dict=myDict2)))\n",
    "print('[df(1)/dtheta1,df(2)/dtheta1] = ' + str(sess.run(df_dtheta1,feed_dict=myDict3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pyBlYxVU6yc"
   },
   "source": [
    "Coupled with gradient descent methods, this gives an incredibly simple way to fit models to data. Below we show how to implement a simple gradient descent. As an example, we recalibrate the slope of the linear function to match data sampling the function $f(x) = x+3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpKDERUvT8_Z"
   },
   "outputs": [],
   "source": [
    "x_data = np.linspace(0,1,10,dtype=np.float32)\n",
    "y_data = x_data+3\n",
    "RMS_LOSS = tf.sqrt( tf.reduce_mean( (y_data-f_of_x)**2 ) )\n",
    "gradient_direction = tf.gradients(RMS_LOSS,theta1)[0]\n",
    "learning_rate = 0.01\n",
    "gradient_update = theta1.assign(theta1 - learning_rate*gradient_direction) #overwrite the value of theta1 using a gradient descent update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7ARkw38XX6S"
   },
   "source": [
    "Here we take 100 steps and check that it does in fact converge to the correct value of 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ted8IAxGWHUk"
   },
   "outputs": [],
   "source": [
    "for training_step in range(100):\n",
    "  sess.run(gradient_update,feed_dict={x:x_data})\n",
    "  print('(RMS_error, theta1) = (' + str(sess.run(RMS_LOSS,feed_dict={x:x_data})) +','+str(sess.run(theta1,feed_dict={x:x_data})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLwlUtL-XeY3"
   },
   "source": [
    "We will see that there is a price to pay for this flexibility. Gradient descent is incredibly slow - the gradient tells us which direction to move, but not how far. The step length is a \"knob\" we have to pick, and is the first of many, many hyperparameters we will learn about that must be tuned to get good answers. Other classical methods are often vastly faster. Below is a snippet that compares solving a least squares problem by gradient descent vs solving the normal equations. You can adapt this code for the first homework assignment - take a note of the linear algebra operations available in tensorflow since this will be your work horse for manipulating tensors - for others check the [tensorflow API](https://www.tensorflow.org/api_docs). I will do my best to point you toward necessary functions in the first few weeks to minimize the time you spend digging through the API. Other notable ones are tools to reshape/resize tensors ([reshape](https://www.tensorflow.org/api_docs/python/tf/reshape), [expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims)), perform reductions to compute sums/means of tensors ([reduce_sum](https://www.tensorflow.org/api_docs/python/tf/reduce_sum),[reduce_mean](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)), and a transpose operation which can be used to permute indices of tensors ([transpose](https://www.tensorflow.org/api_docs/python/tf/transpose)). To perform dot products, matrix-vector products, and other contractions I suggest you use [einsum](https://www.tensorflow.org/api_docs/python/tf/einsum)); there are other functions but this one can perform any contraction operation once you learn to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogBRn0xbWfgR",
    "outputId": "1688524f-7590-4d26-825f-a75a2a7fd735"
   },
   "outputs": [],
   "source": [
    "P1 = tf.ones(shape=(x_data.shape[0]))\n",
    "P2 = x_data\n",
    "P = tf.stack([P1,P2],axis=1)\n",
    "M = tf.einsum('di,dj->ij',P,P)\n",
    "rhs = tf.einsum('di,d->i',P,y_data)\n",
    "theta_optimal = tf.linalg.solve(M,tf.expand_dims(rhs,-1))[:,0]\n",
    "\n",
    "print('Optimal values for theta1 and theta2: ' + str(sess.run(theta_optimal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp_7bpA2b7Nv"
   },
   "source": [
    "**Assignment 1:** Modify this code to perform maximum likelihood estimation of noisy data, fitting a model of the form: $y \\sim \\mathcal{N}(\\theta_1 x + \\theta_2, \\sigma^2)$. Do this both by building a tensorflow model and applying gradient descent to a maximum likelihood function to solve for $(\\theta_1,\\theta_2,\\sigma^2)$, and also by computing analytically a system of normal equations for $(\\theta_1,\\theta_2)$ and an expression for $\\sigma^2$ by computing $\\nabla MLL = 0$ with pencil and paper."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2Znf24fhpvlnZtsxXCN5H",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
