{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmO74C8f3NLEiHxGoJapwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natrask/ENM5310/blob/main/VAE_markov_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We demonstrate now how to extend a VAE to include *inductive biases* which impose prior knowledge about system dynamics. We consider a trivial version of the pendulum problem presented in class, whereby we are only able to collect indirect measurements of the true dynamics. We consider a pendulum system, where the \"hidden\" data z satisfies the ODE governing the angular rotation of the pendulum.\n",
        "$$\\ddot{z} = - \\lambda z$$\n",
        "But we collect noisy data for the position of the pendulum bob.\n",
        "$$ \\vec{y}(t) = <\\sin z(t), -\\cos z(t)> + \\epsilon$$\n",
        "$$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$$\n",
        "\n",
        "This is a simplified example of trying to infer dynamics from video - here we want to find the $y\\in\\mathbb{R}^2 \\rightarrow z \\in\\mathbb{R}$ map, while the full complexity problem of identifying dynamics from video frames would be a more expensive to solve $y\\in\\mathbb{R}^{N_{\\text{pixels}}} \\rightarrow z \\in\\mathbb{R}$ map.\n",
        "\n",
        "There are two tasks to complete to build the architecture after generating the data set. First, we need to build up the VAE encoder and decoder that will take us back and forth between $y$ and $z$. Then we need to construct the ELBO loss function which will impose our assumed dynamics as a prior."
      ],
      "metadata": {
        "id": "MP96fhyLI_Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse.linalg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "sess = tf.Session(config=config)"
      ],
      "metadata": {
        "id": "_woU4wppOeY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d6889e-5b80-41bb-9179-2855bf000711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are manufacturing our dataset. Note that you can control here the number of samples, as well as the magnitude of the noise we're introducing. This is useful for investigating how robust the method is to noise. For a real problem \"noise\" will not be Gaussian white noise added on top of the signal - it would be e.g. all of the background artifacts captured in the video."
      ],
      "metadata": {
        "id": "t62BDB7AOxmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ndata = 2000 #how many trajectories to train on\n",
        "Ntimes = 50  #how many timesteps of solution to save (important for time discretization)\n",
        "dt = 1./Ntimes\n",
        "z0 = np.random.uniform(-1,1,Ndata)\n",
        "t = np.linspace(0,1,Ntimes)\n",
        "theta_true = 1.0/(2.*np.pi)\n",
        "zdata = np.expand_dims(z0,0)*np.cos((1./theta_true)*np.expand_dims(t,-1)) # this is \"hidden\" and not used during training\n",
        "\n",
        "#generate observable coordinates\n",
        "noise = 0.05 # play with this to add more/less noise\n",
        "noiselessdata = np.stack([np.sin(zdata),-np.cos(zdata)],axis=2)\n",
        "ydata = noiselessdata + np.random.normal(0,noise,[Ntimes,Ndata,2])\n",
        "ydata = ydata.transpose([1,0,2]) # reshape data to match tensorflow model requirements"
      ],
      "metadata": {
        "id": "dM6EOAhePAsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we build up the tensorflow model for the autoencoder, implementing two networks for the encoder and decoder, as well as the reparameterization trick to allow us to backprop into the encoder and train. Note for this toy version ($\\mathbb{R}^2\\rightarrow\\mathbb{R}$) of the problem we can get away with a simple dense neural network. For images ($\\mathbb{R}^{N_{\\text{pixels}}}\\rightarrow\\mathbb{R}$) you would need something more sophisticated here, e.g. a CNN, UNet, or vision transformer."
      ],
      "metadata": {
        "id": "7NM09NTwQQ2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ssT3sUUqAEGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder architecture\n",
        "y_in = tf.placeholder(shape=[None,Ntimes,2],dtype=tf.float64)\n",
        "NNhidden0_e = tf.layers.Dense(20,activation='tanh',dtype=tf.float64)\n",
        "NNhidden1_e = tf.layers.Dense(20,activation='tanh',dtype=tf.float64)\n",
        "NNlinear_e  = tf.layers.Dense(2,activation='linear',dtype=tf.float64)\n",
        "def NN_z(x):\n",
        "  return NNlinear_e(NNhidden1_e(NNhidden0_e(x)))\n",
        "\n",
        "# Reparameterization trick\n",
        "mu_z, logvar_z = tf.split(NN_z(y_in),num_or_size_splits=2,axis=2)\n",
        "eps = tf.random.normal(shape=(1,1),dtype=tf.float64)\n",
        "Z_sample = mu_z + eps * tf.exp(logvar_z * .5)\n",
        "\n",
        "# Decoder architecture\n",
        "NNhidden0_d = tf.layers.Dense(20,activation='tanh',dtype=tf.float64)\n",
        "NNhidden1_d = tf.layers.Dense(20,activation='tanh',dtype=tf.float64)\n",
        "NNlinear_d  = tf.layers.Dense(2,activation='linear',dtype=tf.float64)\n",
        "def NN_y(x):\n",
        "  return NNlinear_d(NNhidden1_d(NNhidden0_d(x)))\n",
        "y_hat = NN_y(Z_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fspJAowaPS4e",
        "outputId": "2ee28ff3-0ab8-4bf1-c5f3-954068d6b2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(x) = 2*x+3\n",
            "f(3) = 9.0\n",
            "f(5) = 13.0\n",
            "[f(1),f(2)] = [5. 7.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will build up the model of the dynamics in the latent space. We need to store a trainable variable for the unknown $\\lambda$ (we will call it $\\theta$ to stress the different between the true parameter and the trainable model parameter). Then, we calculate the prior distribution consistent with an implicit Euler discretization of the dynamics (see todays class notes for the derivation)."
      ],
      "metadata": {
        "id": "dgGUr7FbTZQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainable parameter for unknown dynamics timescale\n",
        "theta_learned = (tf.Variable(1.0,dtype=tf.float64))\n",
        "\n",
        "# Calculation of mean and variance for prior distribution which follows from numerical integration\n",
        "alpha = (1.0/(1.0+theta_learned))\n",
        "mu_prior = alpha*2.*mu_z[:,1:-1,:] - alpha*mu_z[:,0:-2,:]\n",
        "logvar_prior = tf.log( 4.*alpha**2 * tf.exp(logvar_z[:,1:-1,:]) + alpha**2 *tf.exp(logvar_z[:,0:-2,:]) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KTdI4rbRaet",
        "outputId": "0cef7533-fcbf-4970-dccd-7a719bb93f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dfdx(x) = 2\n",
            "df(3)/dtheta1 = 2.0\n",
            "df(5)/dtheta1 = 2.0\n",
            "[df(1)/dtheta1,df(2)/dtheta1] = [2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we include expressions for the ELBO loss. Here, the choices that we made for model all terms as Gaussians allow us to use analytic expressions for all of the requisite expectations, and we are left with a nice expression with no estimators.\n",
        "\n",
        "Note that following the $\\beta-$VAE paper, we introduce a constant $\\beta$ which allows us to dial back how strongly we enforce the physics. Take a look here if you want to see the [original $\\beta$-VAE paper](https://), and [take a look here](https://lilianweng.github.io/posts/2018-08-12-vae/) for further reading on autoencoders. We will see through experiments that if we enforce our prior too tightly we can't get a good match to the data."
      ],
      "metadata": {
        "id": "1pyBlYxVU6yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_Q = -0.5*(tf.cast(tf.log(2.0*np.pi),tf.float64) + logvar_z[:,2:,:])\n",
        "KL_1      = -0.5*(tf.cast(tf.log(2.0*np.pi),tf.float64) + logvar_prior)\n",
        "KL_2      = -0.5*( tf.exp( logvar_z[:,2:,:] - logvar_prior ) )\n",
        "KL_3      = -0.5*(mu_z[:,2:,:]-mu_prior)**2 * tf.exp(-logvar_prior)\n",
        "KL_term   = -tf.reduce_sum(KL_1 + KL_2 + KL_3 - Entropy_Q)\n",
        "\n",
        "beta = tf.placeholder(shape = (), dtype=tf.float64)\n",
        "Recon_term = -tf.reduce_sum((y_hat-y_in)**2) #MC estimator for E[log p(x|z)]\n",
        "LOSS = -(Recon_term - beta*KL_term) #maximize so multiply  by -1\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(LOSS)\n",
        "sess.run(tf.global_variables_initializer()) #initialize model"
      ],
      "metadata": {
        "id": "BpKDERUvT8_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we train the model and plot for a randomly selected trajectory in the dataset whether we get a good match to the training data."
      ],
      "metadata": {
        "id": "D7ARkw38XX6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "  data_dict = {y_in:ydata[np.random.randint(0,Ndata,50),:,:],beta:1e-2}\n",
        "  sess.run(optimizer,feed_dict=data_dict)\n",
        "  print(sess.run((LOSS,theta_learned),feed_dict=data_dict))"
      ],
      "metadata": {
        "id": "Ted8IAxGWHUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {y_in:ydata[np.random.randint(0,Ndata,1),:,:],beta:1.0}\n",
        "input_traj = sess.run(y_in,feed_dict=data_dict)\n",
        "trajectory = sess.run(y_hat,feed_dict=data_dict)\n",
        "plt.scatter(trajectory[0][:,0],trajectory[0][:,1])\n",
        "plt.scatter(input_traj[0][:,0],input_traj[0][:,1])"
      ],
      "metadata": {
        "id": "0Hi5b2OfDn5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}