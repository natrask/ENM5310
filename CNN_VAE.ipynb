{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9nPXK2NtYMrqqoI9agsEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natrask/ENM5310/blob/main/CNN_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm adopting here the [tensorflow tutorial for CNN VAEs](https://www.tensorflow.org/tutorials/generative/cvae) stripping out as much of the extra code as possible. If you're comfortable with TF2 at this point you may find the official tutorial to be more comprehensive so please feel free to refer to that."
      ],
      "metadata": {
        "id": "MP96fhyLI_Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse.linalg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "sess = tf.Session(config=config)"
      ],
      "metadata": {
        "id": "_woU4wppOeY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb7a2558-4d2f-493b-ef69-4b5618bbb3cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the MNIST dataset consisting of a bunch of handwritten digits"
      ],
      "metadata": {
        "id": "v-_7Ce5Vq9zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)"
      ],
      "metadata": {
        "id": "plL3gup8q1G9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Break the dataset into a pile of test and training data"
      ],
      "metadata": {
        "id": "PMFVvWrhrGGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build up the encoder architecture, using the reparameterization trick to generate a sample from the posterior distribution"
      ],
      "metadata": {
        "id": "G951L72krPoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 2\n",
        "npix1d = 28\n",
        "Nbatch = 1\n",
        "input_img = tf.placeholder(dtype=tf.float32,shape=(Nbatch,npix1d,npix1d))\n",
        "encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            # No activation\n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "        ])\n",
        "decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            # No activation\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
        "        ])\n",
        "\n",
        "#Now sample from the encoder to get Z, and decode to get Xhat\n",
        "mu_q, logvar_q = tf.split(encoder(tf.expand_dims(input_img,-1)), num_or_size_splits=2, axis=1)\n",
        "eps = tf.random.normal(shape=mu_q.shape)\n",
        "Zsample = eps * tf.exp(logvar_q * .5) + mu_q\n",
        "Xhat = decoder(Zsample)"
      ],
      "metadata": {
        "id": "x1zXmeXorWcb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build ELBO loss function"
      ],
      "metadata": {
        "id": "1_1KeTcMvpQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=Xhat, labels=tf.expand_dims(input_img,-1))\n",
        "logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "logpz = log_normal_pdf(Zsample, 0., 0.)\n",
        "logqz_x = log_normal_pdf(Zsample, mu_q, logvar_q)\n",
        "\n",
        "LOSS = -tf.reduce_mean(logpx_z + logpz - logqz_x)"
      ],
      "metadata": {
        "id": "c4XCufi4v91n"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oN35ED6ovSHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}